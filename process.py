"""
This project provides a demo of a confidential workload to find patients for clinical trials.
The confidential workload handles 2 events: one to trigger the data holders' data quality checks, one to trigger the query of participants candidates to a clinical trial based on specific criteria
"""

import logging
import time
import yaml
import os
import json
import duckdb
from datetime import datetime
import base64
import shutil

from dv_utils import default_settings, Client, ContractManager,SecretManager,audit_log,LogLevel


import pandas as pd

logger = logging.getLogger(__name__)


# let the log go to stdout, as it will be captured by the cage operator
logging.basicConfig(
    level=default_settings.log_level,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

# define an event processing function
def event_processor(evt: dict):
    """
    Process an incoming event
    Exception raised by this function are handled by the default event listener and reported in the logs.
    """
    
    logger.info(f"Processing event {evt}")

    # dispatch events according to their type
    evt_type =evt.get("type", "")
    if evt_type == "CHECK_DATA_QUALITY":
        # use the CHECK_DATA_QUALITY event processor dedicated function
        logger.info(f"Use the check data quality event processor")
        check_data_quality_contracts_event_processor(evt)
    elif(evt_type == "QUERY"):
        process_query_event(evt)
    else:
        generic_event_processor(evt)


def generic_event_processor(evt: dict):
    # push an audit log to reccord for an event that is not understood
    logger.info(f"Received an unhandled event {evt}")

def check_data_quality_contracts_event_processor(evt: dict):
    #audit logs are generated by the dv_utils sdk
    try:
        contractManager=ContractManager()
        test_results=contractManager.check_contracts_for_collaboration_space(default_settings.collaboration_space_id)

        #get data contracts from all data consumers
        data_contracts=contractManager.get_contracts_for_collaboration_space(default_settings.collaboration_space_id,Client.DATA_CONSUMER_COLLABORATOR_ROLE_VALUE)
        #Create in memory duckdb (encrypted memory on confidential computing)
        con = duckdb.connect(database=":memory:")
        
        #Add connector settings to duckdb con for all data contracts and export test results linked to the right client
        model_key="quality_checks"
        for data_contract in data_contracts:
            for test_result_descriptor_id in test_results:
                con = data_contract.connector.add_duck_db_connection(con)
                con.sql(data_contract.export_contract_to_sql_create_table(model_key))
                check_results=test_results[test_result_descriptor_id]
                for check_result in check_results:
                    description=check_result
                    timestamp=now = datetime.now()
                    formated_now = now.strftime('%Y-%m-%dT%H:%M:%SZ')
                    check_result_json=test_results[test_result_descriptor_id][check_result]
                    hasErrors=check_result_json["hasErrors"]
                    hasWarnings=check_result_json["hasWarnings"]
                    hasFailures=check_result_json["hasFailures"]
                    query="INSERT INTO "+model_key+" VALUES ('"+description+"','"+formated_now+"',"+str(hasErrors)+","+str(hasWarnings)+","+str(hasFailures)+",'"+str(json.dumps(check_result_json).replace("'","''"))+"')"
                    con.sql(query)
        data_contract.connector.export_signed_output_duckdb(model_key,default_settings.collaboration_space_id)
    except Exception as e:
        logger.error(e)

def process_query_event(evt: dict):
    """
    Train an XGBoost Classifier model using the logic given in 
     """

    logger.info(f"--------------------------------------------------")
    logger.info(f"|               START BENCHMARKING               |")
    logger.info(f"|                                                |")
    # load the training data from data providers
    # duckDB is used to load the data and aggregated them in one single datasets
    logger.info(f"| 1. Load data from data providers               |")
    logger.info(f"|    https://github.com/./demographic.parquet |")
    logger.info(f"|    https://github.com/./patients.parquet |")
    dataProvider1URL="https://github.com/datavillage-me/cage-process-clinical-trial-patient-cohort-selection/raw/main/data/demographic.parquet"
    #dataProvider1URL="data/demographic.parquet"
    dataProvider2URL="https://github.com/datavillage-me/cage-process-clinical-trial-patient-cohort-selection/raw/main/data/patients.parquet"
    #dataProvider2URL="data/patients.parquet"
    start_time = time.time()
    logger.info(f"|    Start time:  {start_time} secs |")
    
    whereClause=evt.get("parameters", "")
    if whereClause!='':
        baseQuery="SELECT COUNT(*) as total from '"+dataProvider1URL+"' as demographic,'"+dataProvider2URL+"' as patients WHERE demographic.national_id=patients.national_id AND "+whereClause
    else:
        baseQuery="SELECT COUNT(*) as total from '"+dataProvider1URL+"' as demographic,'"+dataProvider2URL+"' as patients WHERE demographic.national_id=patients.national_id"
    
    #total candidates
    df = duckdb.sql(baseQuery).df()
    totalCandidates=df['total'][0]
    print(totalCandidates)
    
    #gender
    #male
    df = duckdb.sql(baseQuery+ " AND demographic.gender='male'").df()
    totalGenderMale=df['total'][0]
    #female
    df = duckdb.sql(baseQuery+ " AND demographic.gender='female'").df()
    totalGenderFemale=df['total'][0]

    #education_level
    #high_school
    df = duckdb.sql(baseQuery+ " AND demographic.education_level='high_school'").df()
    totalEducationLevelHighSchool=df['total'][0]
    #college
    df = duckdb.sql(baseQuery+ " AND demographic.education_level='college'").df()
    totalEducationLevelCollege=df['total'][0]
    #university
    df = duckdb.sql(baseQuery+ " AND demographic.education_level='university'").df()
    totalEducationLevelUniversity=df['total'][0]


    #employment_status
    #unemployed
    df = duckdb.sql(baseQuery+ " AND demographic.employment_status='unemployed'").df()
    totalEmploymentStatusUnemployed=df['total'][0]
    #employed
    df = duckdb.sql(baseQuery+ " AND demographic.employment_status='employed'").df()
    totalEmploymentStatusEmployed=df['total'][0]
    #student
    df = duckdb.sql(baseQuery+ " AND demographic.employment_status='student'").df()
    totalEmploymentStatusStudent=df['total'][0]
    #retired
    df = duckdb.sql(baseQuery+ " AND demographic.employment_status='retired'").df()
    totalEmploymentStatusRetired=df['total'][0]

    execution_time=(time.time() - start_time)
    logger.info(f"|    Execution time:  {execution_time} secs |")

    logger.info(f"| 2. Save outputs of the collaboration           |")
    # The output file model is stored in the data folder
    
    output= ''' {
    "candidates": '''+str(totalCandidates)+''',
        "gender": {
        "male":'''+str(totalGenderMale)+''',
        "female":'''+str(totalGenderFemale)+'''
        },
        "education_level": {
        "high_school":'''+str(totalEducationLevelHighSchool)+''',
        "college":'''+str(totalEducationLevelCollege)+''',
        "university":'''+str(totalEducationLevelUniversity)+'''
        },
        "employment_status":{
        "unemployed":'''+str(totalEmploymentStatusUnemployed)+''',
        "employed":'''+str(totalEmploymentStatusEmployed)+''',
        "student":'''+str(totalEmploymentStatusStudent)+''',
        "retired":'''+str(totalEmploymentStatusRetired)+'''
        }
    } '''

    #with open('data/my.json', 'w', newline='') as file:
        #file.write(output)

    with open('/resources/outputs/candidates-report.json', 'w', newline='') as file:
        file.write(output)
   
    logger.info(f"|                                                |")
    logger.info(f"--------------------------------------------------")
   

if __name__ == "__main__":
    test_event = {
            "type": "QUERY",
            "parameters": ""
    }
    process_query_event(test_event)